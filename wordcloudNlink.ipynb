{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import *\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize \n",
    "import sys\n",
    "import time\n",
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root=r'C:\\Users\\kimyujin\\Desktop\\2020 3-2í•™ê¸°\\ì¸ê³µì§€ëŠ¥\\sentence_tokenized'\n",
    "gniCorpus=PlaintextCorpusReader(corpus_root,'.*\\.txt',encoding='utf-8')\n",
    "year=''\n",
    "YEAR=''\n",
    "li=[]\n",
    "gniRaw=''\n",
    "\n",
    "\n",
    "keyword_list=[]\n",
    "searchKwd=''\n",
    "findList=[]\n",
    "\n",
    "plus_stopwords_to_remove = ['\\\\n','sample', '\\n\\n', 'n', 'Fig','number','study','analysis', '\\\\', 'Title:','result','Title','using','used','Table','based'] \n",
    "STOPWORDS = STOPWORDS.union(set(plus_stopwords_to_remove))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr_3(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "\n",
    "def print_color_3(t):\n",
    "    display(html_print(' '.join([cstr_3(ti, color=ci) for ti,ci in t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_color_3((('hello my name is', 'black'),('jhjfd','red')))\n",
    "#print_color_3((('hello my name is', 'hotpink'),))\n",
    "#a=input('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choban():\n",
    "    \n",
    "    global year\n",
    "    global YEAR\n",
    "    global li\n",
    "    global gniRaw\n",
    "    global stopwords_made\n",
    "    global STOPWORDS\n",
    "    \n",
    "    \n",
    "    \n",
    "    print_color_3((('ğŸ’â€â™€ï¸ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ì—°ë„ë¥¼ ì ì–´ì£¼ì„¸ìš”. EX]2011 \\n:', 'hotpink'),))\n",
    "    year=input('')\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        if ((year.startswith('20')) and (year>='2000' or year<='2020')):\n",
    "            YEAR=str(int(year)-2002)\n",
    "        else:\n",
    "            print_color_3((('ğŸ’â€â™€ï¸ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ì—°ë„ë¥¼ ì ì–´ì£¼ì„¸ìš”. EX]2011 \\n:', 'hotpink'),))\n",
    "            year=input('')\n",
    "\n",
    "    if(YEAR>='10'):\n",
    "        hist=[]\n",
    "        for i in range(0,len(gniCorpus.fileids())):\n",
    "            if((gniCorpus.fileids()[i][4:6])==YEAR):\n",
    "                    hist.append(gniCorpus.fileids()[i])\n",
    "                    li=hist\n",
    "    \n",
    "    gniRaw=gniCorpus.raw(li)\n",
    "\n",
    "    stopwords_made = set(STOPWORDS)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def showingWordcloud():\n",
    "    global keyword_list\n",
    "    wordcloud = WordCloud(\n",
    "                    width = 1600, height = 1400,\n",
    "                    background_color='white',\n",
    "                    max_words=400,\n",
    "                    collocations=False,\n",
    "                    stopwords = stopwords_made, \n",
    "                    min_font_size = 30,\n",
    "                    #mask = gni_mask\n",
    "                    ).generate(gniRaw)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    keyword_list=list(wordcloud.words_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def appearWord():\n",
    "    global searchKwd\n",
    "    print_color_3((('ğŸ’â€â™€ï¸'+year+'ë…„ G&Ië…¼ë¬¸ì— ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 20ê°œëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”!\\n', 'hotpink'),))\n",
    "    #print('ğŸ’â€â™€ï¸'+year+'ë…„ G&Ië…¼ë¬¸ì— ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 20ê°œëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”!\\n')\n",
    "\n",
    "    Ranklist = ['Rank '+str(n+1) for n in range(20)]\n",
    "    KeynManylist = []\n",
    "    for i in range(0,20):\n",
    "            KeynManylist.append([keyword_list[i][0],round(keyword_list[i][1],3)])\n",
    "\n",
    "    df = pd.DataFrame(data = np.array(KeynManylist),\n",
    "                      index = Ranklist, \n",
    "                      columns = ['í‚¤ì›Œë“œ', 'ë¹ˆë„']  )\n",
    "\n",
    "    display(df)\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "       return '<a href=\"{}\">{}</a>'.format(val, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSentence():\n",
    "    global searchKwd\n",
    "    a=0\n",
    "    global findList\n",
    "    findList=[]\n",
    "    \n",
    "    if(searchKwd=='ì—†ìŒ'):\n",
    "        print('ğŸ’â€â™€ï¸í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°')\n",
    "        sys.exit(1)\n",
    "    \n",
    "    for k in range(0,20):\n",
    "        if(searchKwd == keyword_list[k][0]):\n",
    "            a+=1\n",
    "            break\n",
    "            \n",
    "    if a==1:       \n",
    "        for i in range(0,len(li)):\n",
    "            globals()['rawFor{}'.format(i)]=gniCorpus.raw(li[i])\n",
    "            if searchKwd in globals()['rawFor{}'.format(i)]:\n",
    "                rawfile=globals()['rawFor{}'.format(i)]\n",
    "                findkeySent=sent_tokenize(rawfile)\n",
    "                #print(findkeySent)\n",
    "                paperTitle = (((findkeySent[0].split('Title: '))[1]).split('\\n\\n'))[0]\n",
    "                if(year=='2018'): #ì—°ë„ë³„ë¡œ url í˜•ì‹ ë‹¤ë¦„ í•˜í•˜,, \n",
    "                    findList.append(dict(ë…¼ë¬¸ëª…=paperTitle,ë“±ì¥íšŸìˆ˜=rawfile.count(searchKwd),url='https://genominfo.org/upload/pdf/'+'gi-'+year+'-'+(li[i].split('.txt')[0]).split('gni-')[1]+'.pdf'))\n",
    "        print(\"\\n\\n<< \"+searchKwd+'ê°€(ì´) ì–¸ê¸‰ëœ ë…¼ë¬¸ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.>>')\n",
    "        \n",
    "        \n",
    "        \n",
    "             \n",
    "    else:\n",
    "        print_color_3((('ğŸ’â€â™€ï¸í‚¤ì›Œë“œ ì…ë ¥ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ìš”. ìœ„ì˜ 20ê°œ ì¤‘ì—ì„œ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\\n', 'hotpink'),))\n",
    "        searchKwd=input('')\n",
    "        findSentence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalResult_wordcloud():\n",
    "    \n",
    "    global searchKwd\n",
    "    showingWordcloud()\n",
    "    appearWord()\n",
    "\n",
    "    print_color_3((('ğŸ’â€â™€ï¸[í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì°¾ê¸°]\\n'+'ìœ„ì˜ 20ê°œ ì¤‘ ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! '+year+'ë…„ì˜ ë…¼ë¬¸ ì¤‘ í‚¤ì›Œë“œë¥¼ ì–¸ê¸‰í•œ ë…¼ë¬¸ë“¤ì„ ì°¾ì•„ì¤„ê²Œìš”. ex) RNA \\nì°¾ê³ ì‹¶ì€ í‚¤ì›Œë“œê°€ ì—†ë‹¤ë©´ ì—†ìŒ ì´ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.\\n', 'hotpink'),))\n",
    "    searchKwd=input('')\n",
    "    #searchKwd=input('ğŸ’â€â™€ï¸[í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì°¾ê¸°]\\n'+'ìœ„ì˜ 20ê°œ ì¤‘ ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! '+year+'ë…„ì˜ ë…¼ë¬¸ ì¤‘ í‚¤ì›Œë“œë¥¼ ì–¸ê¸‰í•œ ë…¼ë¬¸ë“¤ì„ ì°¾ì•„ì¤„ê²Œìš”. ex) RNA \\nì°¾ê³ ì‹¶ì€ í‚¤ì›Œë“œê°€ ì—†ë‹¤ë©´ ì—†ìŒ ì´ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.\\n')\n",
    "\n",
    "    findSentence()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pleaseshowing(liList):\n",
    "    df = pd.DataFrame(liList,index=[x for x in range(1,len(liList)+1)])\n",
    "    finalshowing=df.style.format({'url': make_clickable}).bar(subset=['ë“±ì¥íšŸìˆ˜'], color='#d65f5f')\n",
    "    #finalshowing\n",
    "    return(finalshowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    choban()\n",
    "    finalResult_wordcloud()\n",
    "   # pleaseshowing(findList)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimi():\n",
    "    global findList\n",
    "    df = pd.DataFrame(findList,index=[x for x in range(1,len(findList)+1)])\n",
    "    finalshowing=df.style.format({'url': make_clickable}).bar(subset=['ë“±ì¥íšŸìˆ˜'], color='#d65f5f')\n",
    "    display(finalshowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:hotpink>ğŸ’â€â™€ï¸ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ì—°ë„ë¥¼ ì ì–´ì£¼ì„¸ìš”. EX]2011 \n",
       ":</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    }
   ],
   "source": [
    "start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.corpus import *\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "n_instances = 500\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "\n",
    "corpus_root=r'C:\\Users\\kimyujin\\Desktop\\2020 3-2í•™ê¸°\\ì¸ê³µì§€ëŠ¥\\sentence_tokenized'\n",
    "gniCorpus=PlaintextCorpusReader(corpus_root,'.*\\.txt',encoding='utf-8')\n",
    "\n",
    "def posORnegORneufunc():\n",
    "    train_subj_docs = subj_docs[:450]\n",
    "    test_subj_docs = subj_docs[450:500]\n",
    "    train_obj_docs = obj_docs[:450]\n",
    "    test_obj_docs = obj_docs[450:500]\n",
    "    training_docs = train_subj_docs+train_obj_docs\n",
    "    testing_docs = test_subj_docs+test_obj_docs\n",
    "    sentim_analyzer = SentimentAnalyzer()\n",
    "    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "\n",
    "    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "\n",
    "    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "\n",
    "    training_set = sentim_analyzer.apply_features(training_docs)\n",
    "    test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "    \n",
    "    trainer = NaiveBayesClassifier.train\n",
    "    classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "    for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "        print('{0}: {1}'.format(key, value))\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    \n",
    "    sentRaw=[]\n",
    "    allList=gniCorpus.fileids()\n",
    "    scoreDic={}\n",
    "\n",
    "    for k in range(0,len(gniCorpus.fileids())):\n",
    "        sentRaw.append(sent_tokenize(gniCorpus.raw(allList[k])))\n",
    "\n",
    "\n",
    "    print('\\në¶„ë¥˜ ì¤‘ ... ì ì‹œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.\\n')\n",
    "    for i in range(0,len(sentRaw)):\n",
    "        totalCompound=0\n",
    "\n",
    "        #print((sentRaw[i][0].split('Title: ')[1]).split('\\n\\n')[0])\n",
    "        text = word_tokenize((sentRaw[i][0].split('Title: ')[1]).split('\\n\\n')[0])\n",
    "        title_pos=nltk.pos_tag(text)\n",
    "        NNP_NNPS=[x[0] for x in title_pos if x[1]=='NNP' or x[1]=='NNPS']\n",
    "        #print(NNP_NNPS)\n",
    "        for j in range(0,len(sentRaw[i])):\n",
    "            ss = sid.polarity_scores(sentRaw[i][j])\n",
    "            totalCompound=totalCompound+ss['compound']\n",
    "\n",
    "        if(totalCompound>0):\n",
    "            for i in NNP_NNPS:\n",
    "                if (i not in scoreDic):\n",
    "                    scoreDic[i]=[1,0]\n",
    "                else:\n",
    "                    scoreDic[i][0]+=1\n",
    "\n",
    "        elif(totalCompound<0):\n",
    "            for i in NNP_NNPS:\n",
    "                if (i not in scoreDic):\n",
    "                    scoreDic[i]=[0,1]\n",
    "                else:\n",
    "                    scoreDic[i][1]+=1\n",
    "                    \n",
    "                    \n",
    "    neg_list=[]\n",
    "    pos_list=[]\n",
    "    result_list=[]\n",
    "    for i in scoreDic.values():\n",
    "        if(i[0]>i[1]):\n",
    "            result_list.append('more Positive')\n",
    "        elif(i[0]<i[1]):\n",
    "            result_list.append('more Negative')\n",
    "        else:\n",
    "            result_list.append('almost Neural')\n",
    "        pos_list.append(i[0])\n",
    "        neg_list.append(i[1])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['word'] = scoreDic.keys()\n",
    "    df['pos'] = pos_list\n",
    "    df['neg'] = neg_list\n",
    "    df['result'] = result_list\n",
    "    df.index = df.index + 1\n",
    "    pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def letstart():\n",
    "    posORnegORneufunc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
